{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import cleaned dataset using load_sets function defined in src.data.sets\n",
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "from src.data.sets import load_sets\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_sets(path='../data/processed/beer_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import `torch`, `torch.nn` as `nn` and `torch.nn.functional` as `F`# Solution:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate `PytorchBinary` with the correct number of input feature and save it into a variable called `model`\n",
    "\n",
    "# Solution:\n",
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "\n",
    "from src.models.pytorch import PytorchMultiClass\n",
    "\n",
    "model = PytorchMultiClass(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PytorchMultiClass(\n",
       "  (layer_1): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (layer_out): Linear(in_features=32, out_features=104, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import `get_device()` from `src.models.pytorch` and set `model` to use the device available\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1,'..')\n",
    "\n",
    "from src.models.pytorch import get_device\n",
    "\n",
    "device = get_device()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PytorchMultiClass(\n",
      "  (layer_1): Linear(in_features=12, out_features=32, bias=True)\n",
      "  (layer_out): Linear(in_features=32, out_features=104, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Print the architecture of `model`\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate a `nn.BCELoss()` and save it into a variable called `criterion` \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate a `torch.optim.Adam()` optimizer with the model's parameters and 0.001 as learning rate and save it into a variable called `optimizer`\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate a torch.optim.lr_scheduler.StepLR() scheduler that will decrease the learning rate by a coefficient of 0.9 for each epoch\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 2 variables called N_EPOCHS and BATCH_SIZE that will take respectively 5 and 32 as values\n",
    "\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dataset and DataLoader from torch.utils.data\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#Import this class from `src/models/pytorch` and convert all sets to PytorchDataset\n",
    "from src.models.pytorch import PytorchDataset\n",
    "\n",
    "train_dataset = PytorchDataset(X=X_train, y=y_train)\n",
    "val_dataset = PytorchDataset(X=X_val, y=y_val)\n",
    "test_dataset = PytorchDataset(X=X_test, y=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\t(train)\t|\tLoss: 0.1206\t|\tAcc: 23.2%\n",
      "\t(valid)\t|\tLoss: 0.1021\t|\tAcc: 23.2%\n",
      "Epoch: 1\n",
      "\t(train)\t|\tLoss: 0.1022\t|\tAcc: 23.2%\n",
      "\t(valid)\t|\tLoss: 0.1022\t|\tAcc: 23.2%\n",
      "Epoch: 2\n",
      "\t(train)\t|\tLoss: 0.1022\t|\tAcc: 23.2%\n",
      "\t(valid)\t|\tLoss: 0.1021\t|\tAcc: 23.2%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpytorch\u001b[39;00m \u001b[39mimport\u001b[39;00m train_classification, test_classification\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_EPOCHS):\n\u001b[1;32m----> 6\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train_classification(train_dataset, model\u001b[39m=\u001b[39;49mmodel, criterion\u001b[39m=\u001b[39;49mcriterion, optimizer\u001b[39m=\u001b[39;49moptimizer, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m      7\u001b[0m     valid_loss, valid_acc \u001b[39m=\u001b[39m test_classification(val_dataset, model\u001b[39m=\u001b[39mmodel, criterion\u001b[39m=\u001b[39mcriterion, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chant\\OneDrive\\Desktop\\Advanced DS for Innovation\\Projects\\Beer_Type_Prediction\\notebooks\\..\\src\\models\\pytorch.py:103\u001b[0m, in \u001b[0;36mtrain_classification\u001b[1;34m(train_data, model, criterion, optimizer, batch_size, device, scheduler, generate_batch)\u001b[0m\n\u001b[0;32m    100\u001b[0m data \u001b[39m=\u001b[39m DataLoader(train_data, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, collate_fn\u001b[39m=\u001b[39mgenerate_batch)\n\u001b[0;32m    102\u001b[0m \u001b[39m# Iterate through data by batch of observations\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m \u001b[39mfor\u001b[39;00m feature, target_class \u001b[39min\u001b[39;00m data:\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m     \u001b[39m# Reset gradients\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    108\u001b[0m     \u001b[39m# Load data to specified device\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\utils\\data\\dataloader.py:624\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 624\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_name):\n\u001b[0;32m    625\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m             \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\autograd\\profiler.py:488\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 488\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_enter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs)\n\u001b[0;32m    489\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\_ops.py:442\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    438\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    439\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    440\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    441\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Create a for loop that will iterate through the specified number of epochs and will train the model with the training set and assess the performance on the validation set and print their scores\n",
    "\n",
    "from src.models.pytorch import train_classification, test_classification\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_classification(train_dataset, model=model, criterion=criterion, optimizer=optimizer, batch_size=BATCH_SIZE, device=device)\n",
    "    valid_loss, valid_acc = test_classification(val_dataset, model=model, criterion=criterion, batch_size=BATCH_SIZE, device=device)\n",
    "\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t(train)\\t|\\tLoss: {train_loss:.4f}\\t|\\tAcc: {train_acc * 100:.1f}%')\n",
    "    print(f'\\t(valid)\\t|\\tLoss: {valid_loss:.4f}\\t|\\tAcc: {valid_acc * 100:.1f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model and NN perform similarly and poorly. Whilst performance improves greatly in comparison to predicting all 104 unique values, the NN still only performs slightly better than the baseline model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model into the models folder\n",
    "\n",
    "torch.save(model.state_dict(), \"../models/neural_network.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.1023\t|\tAccuracy: 0.232\n"
     ]
    }
   ],
   "source": [
    "#Assess the model performance on the testing set and print its scores\n",
    "\n",
    "test_loss, test_acc = test_classification(test_dataset, model=model, criterion=criterion, batch_size=BATCH_SIZE, device=device)\n",
    "print(f'\\tLoss: {test_loss:.4f}\\t|\\tAccuracy: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model=PytorchMultiClass(num_features=12)\n",
    "model.load_state_dict(torch.load(\"../models/neural_network.pt\"))\n",
    "X_train_tensor=torch.Tensor(np.array(X_train))\n",
    "y=model(X_train_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93e5285b5ee05b08152baf054ff49b5b3546c84ca307065b8c0361b70cc13ad6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
